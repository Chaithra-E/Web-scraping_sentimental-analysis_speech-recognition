# Web-scraping_sentimental-analysis_speech-recognition
This project analyzes emotions from YouTube comments and speech using sentiment analysis and speech emotion recognition. Text data is processed with TF-IDF and Logistic Regression, while audio from the TESS dataset uses Wav2Vec2. The multi-modal approach improves emotion detection for potential real-time applications.
This project combines sentiment analysis of YouTube comments with speech emotion recognition to better understand user emotions from both text and audio. YouTube comments were scraped using Apify and analyzed using TF-IDF and Logistic Regression. For speech, the TESS dataset was used, with features extracted via librosa and classified using the Wav2Vec2 model. Both models showed strong performance in identifying emotions. This multi-modal approach enhances emotional insight and has potential for real-time applications and interactive tools

In today’s digital era, the vast amounts of user-generated data on platforms like YouTube present invaluable opportunities to understand public sentiment and emotional responses. Sentiment analysis and speech emotion recognition have become key techniques in natural language processing (NLP) and audio processing that enable machines to interpret human feelings and opinions from textual and vocal cues respectively. 
This project focuses on extracting YouTube comments to perform sentiment analysis and concurrently explores speech emotion recognition to classify emotions from audio data. The integration of these two modalities offers a comprehensive approach to gauge user sentiments more accurately, which is crucial for applications such as social media analytics, customer feedback analysis, and enhancing human-computer interactions.
The motivation behind this project lies in the growing need for automated systems that can interpret the subjective and affective dimensions of human communication across multiple data forms. While text-based sentiment analysis provides explicit insight into user opinions, speech emotion recognition captures subtle emotional nuances through vocal intonation and expression. Together, these approaches facilitate a richer understanding of user engagement and emotional states.

METHODOLOGY
The project is divided into two primary modules: sentiment analysis of YouTube comments and speech emotion recognition from audio samples.

SENTIMENT ANALYSIS
The first step involved collecting and preprocessing a large dataset of YouTube comments. To automate and scale the extraction process, Apify, a powerful web scraping and automation platform, was used to scrape user comments from targeted YouTube videos. The scraped comments were stored in Excel format, and unnecessary metadata columns were removed to streamline the data. A customized text preprocessing pipeline was applied, including cleaning special characters, converting text to lowercase, and removing stopwords to ensure uniformity and improve model performance.
The cleaned comments were then transformed into numerical features using Term Frequency-Inverse Document Frequency (TF-IDF) vectorization, which highlights the importance of distinctive words in each comment relative to the overall corpus. Sentiment labels were assigned automatically using TextBlob’s polarity scoring, categorizing each comment as positive, negative, or neutral. A Logistic Regression classifier was trained on the TF-IDF features to predict sentiment classes. Model performance was evaluated using standard metrics such as accuracy, precision, recall, and F1-score, along with classification reports and confusion matrices. Data visualizations, including sentiment distribution graphs and word clouds, were created to intuitively represent sentiment trends and frequently used terms.

SPEECH EMOTION RECOGNITION
The audio analysis component utilized the Toronto Emotional Speech Set (TESS), a publicly available dataset with labeled emotional speech samples. Each audio file was analyzed using the librosa library to extract essential features such as waveforms and spectrograms, offering both temporal and frequency-based insights into emotional expression.
A deep learning approach was employed using Wav2Vec2, a state-of-the-art pre-trained transformer model optimized for audio representation learning. The model was fine-tuned using the PyTorch framework on the TESS dataset, leveraging GPU support to accelerate training. The network learned to classify emotions—such as anger, fear, happiness, and sadness—based on the acoustic features of the speech signal. The visual representations confirmed clear, emotion-specific audio patterns, which helped validate the model’s learning capability.

RESULTS
The sentiment analysis pipeline performed robustly, with the Logistic Regression model effectively distinguishing positive, negative, and neutral sentiments. The use of Apify significantly improved the scalability and automation of comment collection, enabling broader analysis across multiple videos. Visual tools like sentiment bar plots and word clouds highlighted important emotional themes and commonly used words in the YouTube comments.
In the speech emotion recognition task, the Wav2Vec2-based model achieved high classification accuracy, correctly identifying emotions across a diverse set of speakers and recordings. Waveform and spectrogram patterns exhibited distinguishable structures associated with each emotional class, supporting the relevance of the extracted features.
Together, these results demonstrate that combining NLP for text and deep learning for audio processing creates a powerful multi-modal sentiment analysis system. The synergy between the modalities contributes to a deeper and more reliable understanding of user emotions.

CONCLUSION
This project successfully developed an integrated system that extracts and analyzes YouTube comments for sentiment classification while simultaneously performing emotion recognition on speech audio samples. By leveraging Apify for automated comment scraping, and using advanced modeling techniques such as TF-IDF vectorization, Logistic Regression, and Wav2Vec2, the project illustrates the feasibility and value of a multi-modal approach to sentiment and emotion analysis.
The complementary insights drawn from text and speech not only enhance emotional interpretation but also serve real-world applications such as content moderation, social media monitoring, customer service, personalized recommendations, and AI-human interaction. Future work can explore real-time deployment, fusion of audio-text data into joint models, and expanding the system into an interactive, web-based tool for practical usage.
